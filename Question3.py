# -*- coding: utf-8 -*-
"""Untitled23.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eNACW0NpHLdZ4r7uYnuR3Vv3GUvP3d68
"""

!pip install wandb

import wandb
import math

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from keras.datasets import mnist, fashion_mnist
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import warnings
warnings.filterwarnings("ignore")
import seaborn as sns

ImageClasses = ["Pullover","Shirt","Coat","Trouser","Dress","Sandal","Bag","Sneaker","Ankle boot","T-shirt/top"]

from keras.datasets import fashion_mnist
(x_train,y_train),(x_test,y_test) = fashion_mnist.load_data()

wandb.init(project = "Assignment 1" ,name = "Question 1")

fig,axs = plt.subplots(2 , 5 , figsize = (20 , 6))
axs = axs.flatten()
class_images = []
for i in range(10):
  index = np.argmax(y_train == i)
  axs[i].imshow(x_train[index] , cmap = "gray")
  axs[i].set_title(ImageClasses[i])
  Image = wandb.Image(x_train[index] , caption = [ImageClasses[i]])
  class_images.append(Image)
wandb.log({"examples" : class_images})

x_test = x_test / 255.0
x_train = x_train / 255.0
x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=42)

x_valT = np.transpose(x_val.reshape(x_val.shape[0] , x_val.shape[1] * x_val.shape[2]))
x_trainT = np.transpose(x_train.reshape(x_train.shape[0] , x_train.shape[1] * x_train.shape[2]))
x_testT = np.transpose(x_test.reshape(x_test.shape[0] , x_test.shape[1] * x_test.shape[2]))
y_trainT = y_train.reshape(1 , y_train.shape[0])
y_valT = y_val.reshape(1 , y_val.shape[0])
y_testT = y_test.reshape(1 , y_test.shape[0])



class ActivationFunction:
  '''all activation functions are defined here'''
  def sigmoid(x):
    return  1 / (1 + np.exp(-x))
  def softmax(x):
    return np.exp(x) / np.sum(np.exp(x), axis=0)
  def softmax_derivative(self,x):
    return self.softmax(x) * (1-self.softmax(x))
  def sigmoid_derivative(Z):
    s = 1 /(1 + np.exp(-Z))
    dA = s * (1 - s)
    return dA

# a = np.zeros((2,2))
# a = a + np.ones((2,1))
# a = a + np.ones((2,1))

# np.array([1,2]).reshape(1,2)

# a[1,:]

# a[1,:] = a[1,:] + np.array([1,2]).reshape(1,2)

# a[1,:]

# a = np.array([[1,2],[3,4]])
# b = np.array([[4,5],[7,8]])
# a[:,1] = np.multiply(a[:,1],b[1,:])

# a

a = np.array([1,2])
b = np.array([3,6])
a*b

class NeuralNetwork:
  '''n_layers = total number of layers 
    n_neurons =  number of neurons in hidden and output layer and input layer
    n_input = number of inputs
    n_outputs = number of outputs
    activation_function = default is sigmoid activation function
    weights = weights of each connecting layer , size = (previous Layer Size , Current Layer Size)
    biases = biases of each connecting layer , size = (Current Layer Size , 1)
    TrainInput = input layer's input'''
  n_layers = 0
  activation_function = ""
  n_input = 0
  n_output = 0
  n_neurons = []
  TrainInput = []
  weights = []
  biases = []
  TrainOutput = []
  parameters = {}
  cache = {}
  grads = {}
  def __init__(self,num_neurons_in_hidden_layers,input,output):
    self.n_layers = len(num_neurons_in_hidden_layers) + 2
    self.activation_function = "sigmoid"
    self.TrainInput = input
    self.TrainOutput = output
    self.n_input = input.shape[0]
    self.n_output = output[0,output.argmax(axis = 1)[0]] + 1
    self.n_neurons = num_neurons_in_hidden_layers
    self.n_neurons.append(self.n_output)
    self.n_neurons.insert(0 , self.n_input)
    self.cache["H0"] = input
    self.cache["A0"] = input
    self.grads = {}

    for l in range(1,self.n_layers):
      self.parameters["W" + str(l)] = np.random.randn(self.n_neurons[l] , self.n_neurons[l - 1]) * 0.01
      self.parameters["b" + str(l)] = np.zeros((self.n_neurons[l] , 1)) 


    
  # def linear_forward(self,H,W,b):
  #   A = np.dot(W,H) + b
  #   cache = (H,W,b)
  #   return A,cache
  
  # def active_forward(self,H_prev, W ,b):
  #   print("hello" , H_prev.shape)
  #   A = np.dot(W , H) + b
  #   self.cache["A"]
  #   H , active_cache = ActivationFunction.sigmoid(A)
  #   cache = (linear_cache , active_cache)
  #   return H , cache

  def output(self,A):
    return ActivationFunction.softmax(A)

  
  def forward(self,X):
    self.cache["H0"] = X
    for l in range(1 , self.n_layers):
      H = self.cache["H" + str(l - 1)]
      # print(H.shape)
      W = self.parameters["W" + str(l)]
      b = self.parameters["b" + str(l)]
      A = np.dot(W , H) + b
      # print(W.shape)
      # print(b.shape)
      self.cache["A" + str(l)] = A
      H = ActivationFunction.sigmoid(A)
      self.cache["H" + str(l)] = H
    yPredicted = self.output(self.cache["A" + str(self.n_layers - 1)])
    return yPredicted

  def predict(self,input):
    H = input
    for l in range(1 , self.n_layers - 1):
      # print(H.shape)
      W = self.parameters["W" + str(l)]
      b = self.parameters["b" + str(l)]
      A = np.dot(W , H) + b
      # print(W.shape)
      # print(b.shape)
      H = ActivationFunction.sigmoid(A)

    W = self.parameters["W" + str(self.n_layers - 1)]
    b = self.parameters["b" + str(self.n_layers - 1)]
    A = np.dot(W , H) + b
    y_predicted = self.output(A)
    return y_predicted

  def lossFunction(self,input,TrueOutput,PredictedOutput,loss,batch_size):
    lossValue = 0
    if loss == 'categorical_crossentropy':
    # print(TrueOutput.shape)
    # print(PredictedOutput.shape)
      for i in range(input.shape[1]):
        a = -math.log(PredictedOutput[TrueOutput[0][i],i],2)
        lossValue = lossValue + a
      lossValue = lossValue / batch_size
      print(lossValue)

    elif loss == 'mse':
      e_y = np.transpose(np.eye(self.n_output)[TrueOutput])
      lossValue = (1 / 2) *  np.sum((e_y - PredictedOutput) **2) / batch_size
    return lossValue
  def accuracy(self,input,TrueOutput):
    PredictedOutput = self.predict(input)
    PredictedOutput = np.argmax(PredictedOutput,axis = 0)
    count = 0
    for i in range(TrueOutput.shape[1]):
      if TrueOutput[0,i] == PredictedOutput[i]:
        count = count + 1
    accu = (count / TrueOutput.shape[1]) * 100
    return accu
  # def backprop(self):
  #   e_y = np.transpose(np.eye(self.n_output)[self.TrainOutput[0]])
  #   yPredicted,cache = self.predict(self.TrainInput)
  #   a = -(e_y - yPredicted)
  #   for l in reversed(range(self.n_layers)):
  #     H,A,W,b = cache[l]

  # def linear_backward(dA, cache):
  #   linear_cache , activation_cache = cache
  #   H_prev , W , b = linear_cache
  #   dW = np.dot(dA , H_prev)
  #   db = dA
  #   dH_prev = np.dot(W.T,dA)
  #   dA_prev = np.multiply(dH_prev , ActivationFunction.sigmoid_derivative(activation_cache))
  # def backprop(self,):


  def backprop(self, yPredicted , e_y , batchSize, lamb, loss):
    if loss == 'categorical_crossentropy':
      dA = yPredicted - e_y
    elif loss == 'mse':
      dA = (yPredicted - e_y) * ActivationFunction.softmax_derivative(self.cache["A" + str(self.n_layers - 1)])
    m = dA.shape[1]
    self.grads["dA" + str(self.n_layers - 1)] = dA
    for k in (range(self.n_layers - 1,0,-1)):
      # print(k)
      dA = self.grads["dA" + str(k)]
      H_prev = self.cache["H" + str(k - 1)]
      A_prev = self.cache["A" + str(k - 1)]
      W = self.parameters["W" + str(k)]
      dW = np.zeros(W.shape)
      db = np.zeros((W.shape[0],1))
      dH_prev = np.zeros(H_prev.shape)
      dA_prev = np.zeros(A_prev.shape)

      dW = (np.dot(dA, H_prev.T) + lamb * W) / batchSize
      db = np.sum(dA , axis = 1,keepdims = True)/batchSize
      if k > 1:
        dH_prev = np.matmul(W.T , dA)
        dA_prev = dH_prev * ActivationFunction.sigmoid_derivative(A_prev)

      self.grads["dA" + str(k - 1)] = dA_prev
      self.grads["dW" + str(k)] = dW
      self.grads["db" + str(k)] = db

      # print(H_prev.shape)
      # print(A_prev.shape)
      # print(W.shape)
      # print(dW.shape)
      # print(db.shape)
      # print(dH_prev.shape)
      # print(sigmoid_backward_A_prev.shape)
      # print(dA.shape)
      # print(H_prev_transpose[0,:].shape)

      # print(dW.shape)
      # print(db)
      # print(dA_temp)
      # print(db.shape)
      # print(db)
      # print("error     " , dA_temp.shape)
      # print(np.dot(W.T,dA_temp)[:,0].shape)
      # print(dW.shape)
      # print(db.shape)
      # print(dH_prev.shape)
      # print(dA.shape)

    # print(self.grads)
    return







  def update_parameters_sgd(self,learning_rate):
    for l in range(1 ,self.n_layers):
      self.parameters["W" + str(l)] = self.parameters["W" + str(l)] - learning_rate * self.grads["dW" + str(l)]
      self.parameters["b" + str(l)] = self.parameters["b" + str(l)] - learning_rate * self.grads["db" + str(l)]
    return
  

  def update_parameters_momentum(self,learning_rate,beta,previous_updates):
    for l in range(1 ,self.n_layers):
      previous_updates["W" + str(l)] = beta * previous_updates["W" + str(l)] + self.grads["W" + str(l)]
      previous_updates["b" + str(l)] = beta * previous_updates["b" + str(l)] + self.grads["b" + str(l)]
      self.parameters["W" + str(l)] = self.parameters["W" + str(l)] - learning_rate * previous_updates["dW" + str(l)]
      self.parameters["b" + str(l)] = self.parameters["b" + str(l)] - learning_rate * previous_updates["db" + str(l)]
    return previous_updates

  # def gradient_descent(self,learning_rate,max_iter):
  #   lossList = []
  #   maxIteration = max_iter
  #   for iter in range(maxIteration):
  #     yPredicted = self.forward()
  #     e_y = np.transpose(np.eye(self.n_output)[self.TrainOutput[0]])
  #     dA = self.output(self.cache["A" + str(self.n_layers - 1)]) - e_y
  #     self.backprop(dA)
  #     lossList.append(self.lossFunction(self.TrainInput,self.TrainOutput,yPredicted))
  #     self.update_parameters_sgd(learning_rate)
  #   return lossList



  def fit(self,learning_rate = 0.001,optimizer = 'sgd',batch_size = 500,loss = 'categorical_crossentropy',epochs = 20,lamb = 0):

    loss = 'categorical_crossentropy' 
    costPerEpoch = []
    previous_updates = {}
    for l in range(1 , self.n_layers):
      previous_updates["W" + str(l)] = np.zeros((self.n_neurons[l] , self.n_neurons[l - 1]))
      previous_updates["b" + str(l)] = np.zeros((self.n_neurons[l] , 1))
    t = 1
    v = previous_updates.copy()
    m = previous_updates.copy()
    params_lookAhead = self.parameters.copy()
    count = 1
    beta = 0.9


    for count in range(epochs):
      for i in range(0 , self.TrainInput.shape[1],batch_size):
        batch_count = batch_size
        if i + batch_size > self.TrainInput.shape[1]:
          batch_count = self.TrainInput.shape[1] - i + 1

        if optimizer == 'nadam':
          pass
        elif optimizer == 'nesterov_accelerated_learning':
          for l in range(1 ,self.n_layers):
            self.parameters["W" + str(l)] = self.parameters["W" + str(l)] - beta * previous_updates["W" + str(l)]
            self.parameters["b" + str(l)] = self.parameters["b" + str(l)] - beta * previous_updates["b" + str(l)]
            yPredicted = self.forward(self.TrainInput[:,i:i + batch_size])
            e_y = np.transpose(np.eye(self.n_output)[self.TrainOutput[0,i : i + batch_size]])
            self.backprop(yPredicted,e_y,batch_size,lamb,loss)
            previous_updates["W" + str(l)] = beta * previous_updates["W" + str(l)] + self.grads["dW" + str(l)]
            previous_updates["b" + str(l)] = beta * previous_updates["b" + str(l)] + self.grads["db" + str(l)]
            self.parameters["W" + str(l)] = self.parameters["W" + str(l)] - learning_rate * self.grads["dW" + str(l)]
            self.parameters["b" + str(l)] = self.parameters["b" + str(l)] - learning_rate * self.grads["db" + str(l)]
          
        else:
          yPredicted = self.forward(self.TrainInput[:,i:i + batch_size])
          e_y = np.transpose(np.eye(self.n_output)[self.TrainOutput[0,i : i + batch_size]])
          self.backprop(yPredicted,e_y,batch_size,lamb,loss)
        

          if optimizer == 'sgd':
            self.update_parameters_sgd(learning_rate)
          elif optimizer == 'momentum':
            previous_updates = self.update_parameters_momentum(learning_rate,beta,previous_updates)
          elif optimizer == 'RMSprop':
            previous_updates = self.update_parameters_RMSprop(learning_rate,beta,previous_updates)
          elif optimizer == 'adam':
            v , m , t = self.update_parameters_adam(learning_rate, v ,m , t)

      yPredicted = self.forward(self.TrainInput)
      costPerEpoch.append(self.lossFunction(self.TrainInput,self.TrainOutput,yPredicted,loss,batch_size))


    return costPerEpoch

NN = NeuralNetwork([128,128],x_trainT,y_trainT)

NN.predict(x_trainT)

def accuracy(self,input,TrueOutput):
    PredictedOutput = self.predict(input)
    PredictedOutput = np.argmax(PredictedOutput,axis = 0)
    count = 0
    for i in range(TrueOutput.shape[1]):
      if TrueOutput[0,i] == PredictedOutput[0,i]:
        count = count + 1
    accu = (count / TrueOutput.shape[1]) * 100
    return accu

NN.accuracy(x_trainT,y_trainT)

# NN.cache["H0"].shape

# NN.forward()

# out,caches = NN.predict(x_trainT)

# e_y = np.transpose(np.eye(NN.n_output)[NN.TrainOutput[0]])
# dA = e_y - NN.output(NN.cache["A3"])
# NN.backprop(dA)

# aaa = NN.predict(x_trainT)

# NN.lossFunction(x_trainT,y_trainT,aaa)

lossList = NN.fit(learning_rate = 0.1,epochs = 200,optimizer = 'nesterov_accelerated_learning')

plt.plot(lossList)

PredictedOutput[0]

PredictedOutput = NN.predict(x_trainT)
PredictedOutput = np.argmax(PredictedOutput,axis = 0)
count = 0
for i in range(y_trainT.shape[1]):
  if y_trainT[0,i] == PredictedOutput[i]:
    count = count + 1
accu = (count / y_trainT.shape[1]) * 100

accu