# -*- coding: utf-8 -*-
"""Untitled23.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eNACW0NpHLdZ4r7uYnuR3Vv3GUvP3d68
"""

!pip install wandb

import wandb
import math

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from keras.datasets import mnist, fashion_mnist
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import warnings
warnings.filterwarnings("ignore")
import seaborn as sns

ImageClasses = ["Pullover","Shirt","Coat","Trouser","Dress","Sandal","Bag","Sneaker","Ankle boot","T-shirt/top"]

from keras.datasets import fashion_mnist
(x_train,y_train),(x_test,y_test) = fashion_mnist.load_data()

# wandb.init(project = "Assignment 1" ,name = "Question 1")

# fig,axs = plt.subplots(2 , 5 , figsize = (20 , 6))
# axs = axs.flatten()
# class_images = []
# for i in range(10):
#   index = np.argmax(y_train == i)
#   axs[i].imshow(x_train[index] , cmap = "gray")
#   axs[i].set_title(ImageClasses[i])
#   Image = wandb.Image(x_train[index] , caption = [ImageClasses[i]])
#   class_images.append(Image)
# wandb.log({"examples" : class_images})

x_test = x_test / 255.0
x_train = x_train / 255.0
x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=42)

x_valT = np.transpose(x_val.reshape(x_val.shape[0] , x_val.shape[1] * x_val.shape[2]))
x_trainT = np.transpose(x_train.reshape(x_train.shape[0] , x_train.shape[1] * x_train.shape[2]))
x_testT = np.transpose(x_test.reshape(x_test.shape[0] , x_test.shape[1] * x_test.shape[2]))
y_trainT = y_train.reshape(1 , y_train.shape[0])
y_valT = y_val.reshape(1 , y_val.shape[0])
y_testT = y_test.reshape(1 , y_test.shape[0])

class ActivationFunction:
  '''all activation functions are defined here'''
  def sigmoid(x):
    return  1 / (1 + np.exp(-x))
  def softmax(x):
    return np.exp(x) / np.sum(np.exp(x), axis=0)
  def Relu(x):
    return np.maximum(x,0)
  def tanh(x):
    return np.tanh(x)
  def softmax_derivative(x):
    return ActivationFunction.softmax(x) * (1-ActivationFunction.softmax(x))
  def sigmoid_derivative(Z):
    s = 1 /(1 + np.exp(-Z))
    dA = s * (1 - s)
    return dA
  def Relu_derivative(x):
    return 1*(x > 0)
  def tanh_derivative(x):
    return (1 - (np.tanh(x)**2))

class NeuralNetwork:
  '''n_layers = total number of layers 
    n_neurons =  number of neurons in hidden and output layer and input layer
    n_input = number of inputs
    n_outputs = number of outputs
    activation_function = default is sigmoid activation function
    weights = weights of each connecting layer , size = (previous Layer Size , Current Layer Size)
    biases = biases of each connecting layer , size = (Current Layer Size , 1)
    TrainInput = input layer's input'''
  mode_of_initialization = ""
  n_layers = 0
  activation_function = ""
  n_input = 0
  n_output = 0
  n_neurons = []
  TrainInput = []
  TrainOutput = []
  ValInput = []
  ValOutput = []
  parameters = {}
  cache = {}
  grads = {}
  def __init__(self,mode_of_initialization,number_of_hidden_layers,num_neurons_in_hidden_layers,activation,TrainInput,TrainOutput,ValInput,ValOutput):
    self.mode_of_initialization = mode_of_initialization
    neuronsPerLayer = []
    for i in range(number_of_hidden_layers):
      neuronsPerLayer.append(num_neurons_in_hidden_layers)
    self.n_layers = number_of_hidden_layers + 2
    self.activation_function = activation
    self.TrainInput = TrainInput
    self.TrainOutput = TrainOutput
    self.n_input = TrainInput.shape[0]
    self.n_output = TrainOutput[0,TrainOutput.argmax(axis = 1)[0]] + 1
    self.n_neurons = neuronsPerLayer
    self.n_neurons.append(self.n_output)
    self.n_neurons.insert(0 , self.n_input)
    self.cache["H0"] = TrainInput
    self.cache["A0"] = TrainInput
    self.grads = {}
    self.ValInput = ValInput
    self.ValOutput = ValOutput
    for l in range(1,self.n_layers):
      if self.mode_of_initialization == "random_normal":
        self.parameters["W" + str(l)] = np.random.randn(self.n_neurons[l] , self.n_neurons[l - 1]) 
      elif self.mode_of_initialization == "random_uniform":
        self.parameters["W" + str(l)] = np.random.rand(self.n_neurons[l] , self.n_neurons[l - 1]) 
      elif self.mode_of_initialization == "xavier":
        limit = np.sqrt(2 / float(self.n_neurons[l - 1] + self.n_neurons[l]))
        self.parameters["W" + str(l)] = np.random.normal(0.0, limit, size=(self.n_neurons[l],self.n_neurons[l - 1]))
      self.parameters["b" + str(l)] = np.zeros((self.n_neurons[l] , 1)) 


  def output(self,A):
    return ActivationFunction.softmax(A)

  
  def forward(self,X,activation,parameters):
    self.cache["H0"] = X
    for l in range(1 , self.n_layers):
      H = self.cache["H" + str(l - 1)]
      # print(H.shape)
      W = self.parameters["W" + str(l)]
      b = self.parameters["b" + str(l)]
      A = np.dot(W , H) + b
      # print(W.shape)
      # print(b.shape)
      self.cache["A" + str(l)] = A
      if activation == 'sigmoid':
         H = ActivationFunction.sigmoid(A)
      elif activation == 'relu':
         H = ActivationFunction.Relu(A)
      elif activation == 'tanh':
         H = ActivationFunction.tanh(A)
      self.cache["H" + str(l)] = H
    yPredicted = self.output(self.cache["A" + str(self.n_layers - 1)])
    return yPredicted

  def predict(self,input,parameters):
    H = input
    for l in range(1 , self.n_layers - 1):
      # print(H.shape)
      W = self.parameters["W" + str(l)]
      b = self.parameters["b" + str(l)]
      A = np.dot(W , H) + b
      # print(W.shape)
      # print(b.shape)
      H = ActivationFunction.sigmoid(A)

    W = self.parameters["W" + str(self.n_layers - 1)]
    b = self.parameters["b" + str(self.n_layers - 1)]
    A = np.dot(W , H) + b
    y_predicted = self.output(A)
    return y_predicted

  def lossFunction(self,input,TrueOutput,PredictedOutput,loss,batch_size):
    lossValue = 0
    # print(PredictedOutput.shape)   (10,54000)
    if loss == 'categorical_crossentropy':
      OneHotOfTrueOutput = np.transpose(np.eye(self.n_output)[TrueOutput[0]])
      sum = -np.sum(OneHotOfTrueOutput * np.log(PredictedOutput + 1e-9))
      lossValue = sum / batch_size
      # print(loss)
    return lossValue
  def accuracy(self,input,TrueOutput,PredictedOutput):
    PredictedOutput = np.argmax(PredictedOutput,axis = 0)
    count = 0
    for i in range(TrueOutput.shape[1]):
      if TrueOutput[0,i] == PredictedOutput[i]:
        count = count + 1
    accu = (count / TrueOutput.shape[1]) * 100
    return accu



  def backprop(self, yPredicted , e_y , batchSize,loss,activation,parameters):
    if loss == 'categorical_crossentropy':
      dA = yPredicted - e_y
    m = dA.shape[1]
    self.grads["dA" + str(self.n_layers - 1)] = dA
    for k in (range(self.n_layers - 1,0,-1)):
      # print(k)
      dA = self.grads["dA" + str(k)]
      H_prev = self.cache["H" + str(k - 1)]
      A_prev = self.cache["A" + str(k - 1)]
      W = self.parameters["W" + str(k)]
      dW = np.zeros(W.shape)
      db = np.zeros((W.shape[0],1))
      dH_prev = np.zeros(H_prev.shape)
      dA_prev = np.zeros(A_prev.shape)

      dW = (np.dot(dA, H_prev.T)) / batchSize
      db = np.sum(dA , axis = 1,keepdims = True)/batchSize
      if k > 1:
        dH_prev = np.matmul(W.T , dA)
        if activation == 'sigmoid':
          dA_prev = dH_prev * ActivationFunction.sigmoid_derivative(A_prev)
        elif activation == 'tanh':
          dA_prev = dH_prev * ActivationFunction.tanh_derivative(A_prev)
        elif activation == 'relu':
          dA_prev = dH_prev * ActivationFunction.Relu_derivative(A_prev)
      self.grads["dA" + str(k - 1)] = dA_prev
      self.grads["dW" + str(k)] = dW
      self.grads["db" + str(k)] = db
    return







  def update_parameters_sgd(self,learning_rate):
    for l in range(1 ,self.n_layers):
      self.parameters["W" + str(l)] = self.parameters["W" + str(l)] - learning_rate * self.grads["dW" + str(l)]
      self.parameters["b" + str(l)] = self.parameters["b" + str(l)] - learning_rate * self.grads["db" + str(l)]
    return
  

  def update_parameters_momentum(self,learning_rate,beta,previous_updates):
    for l in range(1 ,self.n_layers):
      previous_updates["W" + str(l)] = beta * previous_updates["W" + str(l)] + self.grads["dW" + str(l)]
      previous_updates["b" + str(l)] = beta * previous_updates["b" + str(l)] + self.grads["db" + str(l)]
      self.parameters["W" + str(l)] = self.parameters["W" + str(l)] - learning_rate * previous_updates["W" + str(l)]
      self.parameters["b" + str(l)] = self.parameters["b" + str(l)] - learning_rate * previous_updates["b" + str(l)]
    return previous_updates



  def update_parameters_RMSprop(self,learning_rate,beta,epsilon,previous_updates):
    for l in range(1 ,self.n_layers):
      previous_updates["W" + str(l)] = beta * previous_updates["W" + str(l)] + (1 - beta) * np.square(self.grads["dW" + str(l)])
      previous_updates["b" + str(l)] = beta * previous_updates["b" + str(l)] + (1 - beta) * np.square(self.grads["db" + str(l)])
      factorW = learning_rate / (np.sqrt(previous_updates["W" + str(l)] + epsilon))
      factorb = learning_rate / (np.sqrt(previous_updates["b" + str(l)] + epsilon))
      self.parameters["W" + str(l)] = self.parameters["W" + str(l)] - factorW * self.grads["dW" + str(l)]
      self.parameters["b" + str(l)] = self.parameters["b" + str(l)] - factorb * self.grads["db" + str(l)]
    return previous_updates

  def update_parameters_adam(self,learning_rate,beta1,beta2,epsilon,M , V , t):
    for l in range(1 ,self.n_layers):
      M["W" + str(l)] = beta1 *M["W" + str(l)] + (1 - beta1) * self.grads["dW" + str(l)]
      M["b" + str(l)] = beta1 *M["b" + str(l)] + (1 - beta1) * self.grads["db" + str(l)]
      V["W" + str(l)] = beta2 *V["W" + str(l)] + (1 - beta2) * np.square(self.grads["dW" + str(l)])
      V["b" + str(l)] = beta2 *V["b" + str(l)] + (1 - beta2) * np.square(self.grads["db" + str(l)])
      MW_corrected = M["W" + str(l)] / (1 - (beta1 ** t))
      Mb_corrected = M["b" + str(l)] / (1 - (beta1 ** t))
      VW_corrected = V["W" + str(l)] / (1 - (beta2 ** t))
      Vb_corrected = V["b" + str(l)] / (1 - (beta2 ** t))
      factorW = learning_rate / (np.sqrt(VW_corrected) + epsilon)
      factorb = learning_rate / (np.sqrt(Vb_corrected) + epsilon)
      self.parameters["W" + str(l)] = self.parameters["W" + str(l)] - factorW * MW_corrected
      self.parameters["b" + str(l)] = self.parameters["b" + str(l)] - factorb * Mb_corrected
    return M,V,t + 1
    

  def update_parameters_NAG(self,input,output,learning_rate,beta,previous_updates,batch_size,loss):
    parameters = {}
    for l in range(1 , self.n_layers):
      parameters["W" + str(l)] = np.zeros((self.n_neurons[l] , self.n_neurons[l - 1]))
      parameters["b" + str(l)] = np.zeros((self.n_neurons[l] , 1))
    for l in range(1 ,self.n_layers):
      parameters["W" + str(l)] = self.parameters["W" + str(l)] - beta * previous_updates["W" + str(l)]
      parameters["b" + str(l)] = self.parameters["b" + str(l)] - beta * previous_updates["b" + str(l)]
    yPredicted = self.forward(input,self.activation_function,parameters)
    e_y = np.transpose(np.eye(self.n_output)[output])
    self.backprop(yPredicted,e_y,batch_size,loss,self.activation_function,parameters)
    for l in range(1 ,self.n_layers):
      previous_updates["W" + str(l)] = beta * previous_updates["W" + str(l)] + self.grads["dW" + str(l)]
      previous_updates["b" + str(l)] = beta * previous_updates["b" + str(l)] + self.grads["db" + str(l)]
      self.parameters["W" + str(l)] = self.parameters["W" + str(l)] - learning_rate * self.grads["dW" + str(l)]
      self.parameters["b" + str(l)] = self.parameters["b" + str(l)] - learning_rate * self.grads["db" + str(l)]
    
    return previous_updates



  def update_parameters_nadam(self,learning_rate,beta1,beta2,epsilon,M , V , t):
    for l in range(1 ,self.n_layers):
      M["W" + str(l)] = beta1 *M["W" + str(l)] + (1 - beta1) * self.grads["dW" + str(l)]
      M["b" + str(l)] = beta1 *M["b" + str(l)] + (1 - beta1) * self.grads["db" + str(l)]
      MW_corrected = M["W" + str(l)] / (1 - (beta1 ** (t)))
      Mb_corrected = M["b" + str(l)] / (1 - (beta1 ** (t)))


      V["W" + str(l)] = beta2 *V["W" + str(l)] + (1 - beta2) * np.square(self.grads["dW" + str(l)])
      V["b" + str(l)] = beta2 *V["b" + str(l)] + (1 - beta2) * np.square(self.grads["db" + str(l)])
      VW_corrected = V["W" + str(l)] / (1 - (beta2 ** (t)))
      Vb_corrected = V["b" + str(l)] / (1 - (beta2 ** (t)))


      factorW = learning_rate / (np.sqrt(VW_corrected) + epsilon)
      factorb = learning_rate / (np.sqrt(Vb_corrected) + epsilon)
      term1 = 1 - (beta1 ** (t))
      term2 = (1 - beta1) * self.grads["dW" + str(l)] / term1
      term3 = (1 - beta1) * self.grads["db" + str(l)] / term1

      self.parameters["W" + str(l)] = self.parameters["W" + str(l)] - factorW * (beta1 * MW_corrected + term2)
      self.parameters["b" + str(l)] = self.parameters["b" + str(l)] - factorb * (beta1 * Mb_corrected + term3)
    return M,V,t + 1

  def fit(self,learning_rate = 0.001,beta = 0.9,beta1 = 0.9,beta2 = 0.999 ,epsilon = 1e-6, optimizer = 'sgd',batch_size = 100,loss = 'categorical_crossentropy',epochs = 20):
    
    TrainCostPerEpoch = []
    TrainCostPerBatch = []
    TrainAccuracyPerEpoch = []
    ValCostPerEpoch = []
    ValAccuracyPerEpoch = []
    previous_updates = {}
    M = {}
    V = {}
    for l in range(1 , self.n_layers):
      previous_updates["W" + str(l)] = np.zeros((self.n_neurons[l] , self.n_neurons[l - 1]))
      previous_updates["b" + str(l)] = np.zeros((self.n_neurons[l] , 1))
    for l in range(1 , self.n_layers):
      M["W" + str(l)] = np.zeros((self.n_neurons[l] , self.n_neurons[l - 1]))
      M["b" + str(l)] = np.zeros((self.n_neurons[l] , 1))
      V["W" + str(l)] = np.zeros((self.n_neurons[l] , self.n_neurons[l - 1]))
      V["b" + str(l)] = np.zeros((self.n_neurons[l] , 1))
    t = 1


    for count in range(epochs):
      for i in range(0 , self.TrainInput.shape[1],batch_size):
        if i + batch_size > self.TrainInput.shape[1]:
          continue


    
        if optimizer == 'nesterov_accelerated_gradient':
          previous_updates = self.update_parameters_NAG(self.TrainInput[:,i:i + batch_size],self.TrainOutput[0,i : i + batch_size],learning_rate,beta,previous_updates,batch_size,loss)
          # print(previous_updates)
        else:
          parameters = self.parameters
          yPredicted = self.forward(self.TrainInput[:,i:i + batch_size],self.activation_function,parameters)
          e_y = np.transpose(np.eye(self.n_output)[self.TrainOutput[0,i : i + batch_size]])
          self.backprop(yPredicted,e_y,batch_size,loss,self.activation_function,parameters)
          if optimizer == 'sgd':
            self.update_parameters_sgd(learning_rate)
          elif optimizer == 'momentum':
            previous_updates = self.update_parameters_momentum(learning_rate,beta,previous_updates)
          elif optimizer == 'RMSprop':
            previous_updates = self.update_parameters_RMSprop(learning_rate,beta,epsilon,previous_updates)
          elif optimizer == 'adam':
            epsilon = 1e-8
            M , V , t= self.update_parameters_adam(learning_rate,beta1,beta2,epsilon,M , V , t)
          elif optimizer == 'nadam':
            epsilon = 1e-8
            M , V , t= self.update_parameters_nadam(learning_rate,beta1,beta2,epsilon,M , V , t)
      
      yPredicted = self.forward(self.TrainInput,self.activation_function,self.parameters)
      train_cost = self.lossFunction(self.TrainInput,self.TrainOutput,yPredicted,loss,self.TrainInput.shape[1])
      TrainCostPerEpoch.append(train_cost)

      valYPredicted = self.forward(self.ValInput,self.activation_function,self.parameters)
      val_cost = self.lossFunction(self.ValInput,self.ValOutput,valYPredicted,loss,self.ValInput.shape[1])
      ValCostPerEpoch.append(val_cost)

      train_acc = self.accuracy(self.TrainInput, self.TrainOutput,yPredicted)
      TrainAccuracyPerEpoch.append(train_acc)

      val_acc = self.accuracy(self.ValInput, self.ValOutput,valYPredicted)
      ValAccuracyPerEpoch.append(val_acc)
      print("********************************")
      print("Epoch Number = {}".format(count))
      print("Training Accuracy = {}".format(TrainAccuracyPerEpoch[-1]))
      print("Validation Accuracy = {}".format(ValAccuracyPerEpoch[-1]))

      #wandb.log({"training_acc": train_acc, "validation_accuracy": val_acc, "training_loss": train_cost, "validation cost": val_cost})
      
    return TrainCostPerEpoch,TrainAccuracyPerEpoch,ValCostPerEpoch,ValAccuracyPerEpoch

# adam 0.5
#nadam 0.01

# NN = NeuralNetwork('xavier',3,128,'relu',x_trainT,y_trainT,x_valT,y_valT)

# TrainCostPerEpoch,TrainAccuracyPerEpoch,ValCostPerEpoch,ValAccuracyPerEpoch = NN.fit(learning_rate = 0.01,epochs = 10,optimizer = 'sgd',loss = 'categorical_crossentropy')

def main():
   wandb.init(project = 'Assignment 1' , name = 'Question 4')
   config = wandb.config
   NN = NeuralNetwork(mode_of_initialization = config.mode_of_initialization,number_of_hidden_layers = config.number_of_hidden_layers , num_neurons_in_hidden_layers = config.num_neurons_in_hidden_layers,activation = config.activation ,TrainInput = x_trainT,TrainOutput = y_trainT,ValInput = x_valT,ValOutput = y_valT)
   NN.fit(learning_rate = config.learning_rate, beta = 0.9 , beta1 = 0.9 , beta2 = 0.999 , epsilon = 1e-6, optimizer = config.optimizer , batch_size = config.batch_size , loss = config.loss , epochs = config.epochs)

sweep_configuration = {
    'method': 'bayes',
    'name': 'ACCURACY VS EPOCH',
    'metric': {
        'goal': 'maximize', 
        'name': 'validation_accuracy'
        },
    'parameters': {
        'mode_of_initialization': {'values': ['xavier','random_normal','random_uniform']},
        'number_of_hidden_layers' : {'values' : [3,4,5]},
        'num_neurons_in_hidden_layers' : {'values' : [32,64,128]},

        'learning_rate': {'values':[0.001,0.0001]},
        'beta' : {'values' : [0.9,0.99,0.999]},
        'optimizer' : {'values' : ['sgd','momentum','RMSprop','adam','nadam','nesterov_accelerated_gradient']},
                
        'batch_size': {'values': [16,32,64]},
        'epochs': {'values': [5,10]},
        'loss' : {'values' : ['categorical_crossentropy','mse']},
        'lamb' : {'values' : [0 , 0.0001 ,0.001,0.01,0.1,0.5]},
        'activation' : {'values' : ['sigmoid','relu','tanh']},
        'weight_decay' : {'values' : [0, 0.0005,0.5]}
       }
    }


#sweep_id = wandb.sweep(sweep = sweep_configuration , project = 'DL_assignment_1')
#wandb.agent(sweep_id , function = main , count = 1)

sweep_id = wandb.sweep(sweep = sweep_configuration , project = 'Assignment 1')

wandb.agent(sweep_id , function = main , count = 50)

wandb

a = NN.predict(x_trainT)

b = np.argmax(a,axis = 0)



def accuracy(self,input,TrueOutput):
    PredictedOutput = self.predict(input)
    PredictedOutput = np.argmax(PredictedOutput,axis = 0)
    count = 0
    for i in range(TrueOutput.shape[1]):
      if TrueOutput[0,i] == PredictedOutput[0,i]:
        count = count + 1
    accu = (count / TrueOutput.shape[1]) * 100
    return accu

NN.accuracy(x_trainT,y_trainT)

NN.accuracy(x_testT,y_testT)

# NN.cache["H0"].shape

# NN.forward()

# out,caches = NN.predict(x_trainT)

# e_y = np.transpose(np.eye(NN.n_output)[NN.TrainOutput[0]])
# dA = e_y - NN.output(NN.cache["A3"])
# NN.backprop(dA)

# aaa = NN.predict(x_trainT)

# NN.lossFunction(x_trainT,y_trainT,aaa)

# adam = 0.001
#rmsprop = 0.01
#else 0.1



PredictedOutput[0]

PredictedOutput = NN.predict(x_trainT)
PredictedOutput = np.argmax(PredictedOutput,axis = 0)
count = 0
for i in range(y_trainT.shape[1]):
  if y_trainT[0,i] == PredictedOutput[i]:
    count = count + 1
accu = (count / y_trainT.shape[1]) * 100

accu

import wandb
wandb.login()